{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyMD0vJ1qk1y"
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reformatting test_data.jsonl from fine-tuning code for two-shot prompting"
      ],
      "metadata": {
        "id": "gKejDQBdqrO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Step 1: Read the JSONL Data\n",
        "with open('test_data.jsonl', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    data = [json.loads(line) for line in lines]\n",
        "\n",
        "# Step 2: Transform the Data\n",
        "grouped_data = []\n",
        "\n",
        "for i in range(0, len(data) - 2, 3):\n",
        "    content_str = (\n",
        "        'Example 1: ' + data[i][\"messages\"][1][\"content\"] + '\\n' +\n",
        "        'Answer 1: ' + data[i][\"messages\"][2][\"content\"] + '\\n' +\n",
        "        'Example 2: ' + data[i+1][\"messages\"][1][\"content\"] + '\\n' +\n",
        "        'Answer 2: ' + data[i+1][\"messages\"][2][\"content\"] + '\\n' +\n",
        "        data[i+2][\"messages\"][1][\"content\"]\n",
        "    )\n",
        "    grouped_example = {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": (\n",
        "                    \"You are an assistant that is meant to determine whether the \"\n",
        "                    \"last comment mentioned in the user message is an example of counterspeech. \"\n",
        "                    \"You have been given two comments before that as examples.\"\n",
        "                    \" Counterspeech is defined as a \"\n",
        "                    \"response or comment that counters hateful or harmful speech/ideas. Respond with true or false.\"\n",
        "                )\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": content_str},\n",
        "            {\"role\": \"assistant\", \"content\": data[i+2][\"messages\"][2][\"content\"]}\n",
        "        ]\n",
        "    }\n",
        "    grouped_data.append(grouped_example)\n",
        "\n",
        "# Save transformed data (optional)\n",
        "with open('two-shot test data.jsonl', 'w') as file:\n",
        "    json.dump(grouped_data, file, indent=4)\n"
      ],
      "metadata": {
        "id": "kN6HaIbPq0DY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Input and output file names\n",
        "input_filename = 'two-shot test data.jsonl'\n",
        "output_filename = 'reformatted_two-shot test data.jsonl'\n",
        "\n",
        "# Load data from the input file\n",
        "with open(input_filename, 'r') as infile:\n",
        "    data = json.load(infile)\n",
        "\n",
        "# Write data to the output file in .jsonl format\n",
        "with open(output_filename, 'w') as outfile:\n",
        "    for item in data:\n",
        "        outfile.write(json.dumps(item) + '\\n')\n",
        "\n",
        "print(f\"Data reformatted and saved to {output_filename}\")"
      ],
      "metadata": {
        "id": "HB_mngprq2ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metrics & Results"
      ],
      "metadata": {
        "id": "_9h-gfnsrcdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from sklearn.metrics import classification_report, accuracy_score, recall_score, f1_score, precision_score\n",
        "\n",
        "# Set API Key\n",
        "os.environ[\"OPENAI_API_KEY\"] = #<insert OpenAI API Key>\n",
        "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Read the .jsonl file\n",
        "data = []\n",
        "with open('reformatted_two-shot test data.jsonl', 'r') as file:\n",
        "    for line in file:\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "# Lists to store actual and predicted values\n",
        "actual = []\n",
        "predicted = []\n",
        "# Go through each example and predict\n",
        "for example in data:\n",
        "    # Extract messages without the assistant's message\n",
        "    messages = example[\"messages\"][:-1]\n",
        "\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=messages,\n",
        "            temperature=0\n",
        "        )\n",
        "        predicted_response = response[\"choices\"][0][\"message\"][\"content\"].lower()\n",
        "\n",
        "        # Check the response and treat any non-true responses as 'false'\n",
        "        if predicted_response not in ['true', 'false']:\n",
        "            if 'true' in predicted_response:\n",
        "              predicted_response = 'true'\n",
        "            elif 'false' in predicted_response:\n",
        "              predicted_response = 'false'\n",
        "        print(predicted_response)\n",
        "\n",
        "        predicted.append(predicted_response)\n",
        "        actual_response = example[\"messages\"][-1][\"content\"].lower()\n",
        "        actual.append(actual_response)\n",
        "    except Exception as e:\n",
        "        print(\"Error for example:\", messages)\n",
        "        print(e)\n",
        "\n",
        " # Wait for 1 second before the next request\n",
        "\n",
        "# Metrics\n",
        "print(\"Accuracy:\", accuracy_score(actual, predicted))\n",
        "print(\"F1-score:\", f1_score(actual, predicted, pos_label=\"true\"))\n",
        "print(\"Precision:\", precision_score(actual, predicted, pos_label=\"true\"))\n",
        "print(\"Recall:\", recall_score(actual, predicted, pos_label=\"true\"))\n",
        "print(classification_report(actual, predicted, target_names=[\"false\", \"true\"]))\n",
        "\n",
        "# Displaying examples\n",
        "correct = []\n",
        "incorrect = []\n",
        "\n",
        "for a, p, ex in zip(actual, predicted, data):\n",
        "    if a == p:\n",
        "        correct.append(ex)\n",
        "    else:\n",
        "        incorrect.append(ex)\n",
        "\n",
        "print(\"\\nExamples GPT-3.5 got right:\")\n",
        "for c in correct:\n",
        "    print(c)\n",
        "\n",
        "print(\"\\nExamples GPT-3.5 got wrong:\")\n",
        "for ic in incorrect:\n",
        "    print(ic)\n"
      ],
      "metadata": {
        "id": "PBYieVIUq4Pm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}